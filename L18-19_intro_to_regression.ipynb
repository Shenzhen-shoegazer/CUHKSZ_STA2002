{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c693df",
   "metadata": {},
   "source": [
    "# Lectures 18 & 19: Introduction to Regression\n",
    "\n",
    "在前面的课程中，我们学习了假设检验和方差分析的方法，这些方法主要用于比较不同组之间的均值差异。然而，在实际应用中，我们经常需要研究变量之间的关系，这就是回归分析的领域。回归分析旨在建立一个数学模型，以描述一个或多个自变量（预测变量）与因变量（响应变量）之间的关系。\n",
    "\n",
    "## Least Squares Regression Line\n",
    "\n",
    "最小二乘回归线是一种用于描述两个变量之间线性关系的统计方法。假设我们有一组数据点$(X_i, Y_i)$，其中$X_i$是自变量，$Y_i$是因变量。我们希望找到一条直线$Y = \\alpha + \\beta X$，使得所有数据点到这条直线的垂直距离的平方和最小，即：$$ \\sum_{i=1}^{n} (Y_i - \\bar{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (\\alpha + \\beta x_i))^2 $$ $$ (\\alpha_{LS}, \\beta_{LS}) = \\underset{\\alpha, \\beta}{\\operatorname{argmin}} \\sum_{i=1}^{n} (y_i - \\alpha - \\beta(x_i - \\bar{x}))^2. $$\n",
    "\n",
    "求偏导数并设为0，得到最小二乘估计量：$$ \\beta_{LS} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}, $$ $$ \\alpha_{LS} = \\bar{y} - \\beta_{LS} \\bar{x}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a48c8f",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Model\n",
    "\n",
    "简单线性回归模型假设因变量$Y$与自变量$X$之间存在线性关系，并且误差项$\\epsilon$服从正态分布。模型表示为：$$ Y_i = \\alpha + \\beta(x_i - \\bar{x}) + \\epsilon_i $$  其中，$Y_i$是服从正态分布的随机变量，$x_i$是观察到的确定自变量值，$\\epsilon_i \\sim N(0, \\sigma^2)$表示误差项。\n",
    "\n",
    "使用MLE进行估计，得到：\n",
    "$$\\hat{\\alpha} = \\alpha_{LS} = \\bar{Y} \\sim N\\left(\\alpha, \\frac{\\sigma^2}{n}\\right)$$\n",
    "$$\\hat{\\beta} = \\beta_{LS} = \\frac{\\sum_{i=1}^{n} Y_i(x_i - \\bar{x})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\sim N\\left(\\beta, \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\right)$$\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\hat{\\alpha} - \\hat{\\beta}(x_i - \\bar{x})]^2$$\n",
    "\n",
    "至于$\\hat{\\sigma}^2$的分布，可以通过以下推导得到：\n",
    "令$Q_1 := \\left( \\frac{\\hat{\\alpha} - \\alpha}{\\sigma/\\sqrt{n}} \\right)^2 \\sim \\chi^2(1)$； $Q_2 := \\left( \\frac{\\hat{\\beta} - \\beta}{\\sigma/\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}} \\right)^2 \\sim \\chi^2(1)$； $Q_3 := \\frac{n\\hat{\\sigma}^2}{\\sigma^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\hat{\\alpha} - \\hat{\\beta}(x_i - \\bar{x}))^2 \\ge 0$\n",
    "\n",
    "再令$Q := \\sum_{i=1}^{n} \\left( \\frac{Y_i - \\alpha - \\beta(x_i - \\bar{x})}{\\sigma} \\right)^2 \\sim \\chi^2(n) = Q_1 + Q_2 + Q_3$\n",
    "\n",
    "因此，$Q_3$独立于$Q_1$和$Q_2$，且$$\\frac{n\\hat{\\sigma}^2}{\\sigma^2} = Q_3 \\sim \\chi^2(n - 2)$$\n",
    "\n",
    "注意：$\\hat{\\sigma}^2$不是无偏的，其无偏估计量为$$S^2 = \\frac{n}{n-2} \\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum_{i=1}^{n} [Y_i - \\hat{\\alpha} - \\hat{\\beta}(x_i - \\bar{x})]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679f791",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "引理：设 $\\hat{\\theta}$ 是参数 $\\theta$ 的点估计量，且 $\\hat{\\theta} \\sim N(\\theta, c\\sigma^2)$，其中 $c>0$ 是一个已知且固定的正常数。 \n",
    "- 如果 $\\sigma^2$ 是已知的，那么 $\\theta$ 的双尾 $100(1-\\alpha)\\%$ 置信区间是 $$ \\hat{\\theta} \\pm \\frac{z_{\\alpha/2}\\sigma}{\\sqrt{c}} $$ \n",
    "- 如果 $\\sigma^2$ 是未知的，设 $S^2$ 是 $\\sigma^2$ 的无偏估计量，且 $S^2$ 与 $\\hat{\\theta}$ 独立，并有 $\\frac{rS^2}{\\sigma^2} \\sim \\chi^2(r)$，其中 $r$ 是一个正整数，那么 $\\theta$ 的双尾$100(1-\\alpha)\\%$ 置信区间是 $$ \\hat{\\theta} \\pm \\frac{t_{\\alpha/2}(r)S}{\\sqrt{c}} $$\n",
    "\n",
    "据此，可得到：\n",
    "- $\\alpha$的$100(1-\\alpha)\\%$置信区间为$$ \\hat{\\alpha} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} $$如果$\\sigma^2$未知，则置信区间为$$ \\hat{\\alpha} \\pm t_{\\alpha/2}(n-2) \\frac{S}{\\sqrt{n}} $$\n",
    "- $\\beta$的$100(1-\\alpha)\\%$置信区间为$$ \\hat{\\beta} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}} $$如果$\\sigma^2$未知，则置信区间为$$ \\hat{\\beta} \\pm t_{\\alpha/2}(n-2) \\frac{S}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}} $$\n",
    "- $\\mu(x) = \\alpha + \\beta(x - \\bar{x})$的置信区间为$$ \\hat{\\mu}(x) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}} $$如果$\\sigma^2$未知，则置信区间为$$ \\hat{\\mu}(x) \\pm t_{\\alpha/2}(n-2) S \\sqrt{\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}} $$\n",
    "  - 其中，$\\hat{\\mu}(x) = \\hat{\\alpha} + \\hat{\\beta}(x - \\bar{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb89dab",
   "metadata": {},
   "source": [
    "## Prediction Intervals\n",
    "\n",
    "线性回归具有预测新观测值的能力。对于给定的自变量值$x$，我们可以预测对应的因变量值$Y$。预测区间考虑了估计误差和未来观测值的变异性。\n",
    "\n",
    "我们希望构建一个区间，该区间能预测： $$Y_{n+1} = \\alpha + \\beta(x_{n+1} - \\bar{x}) + \\epsilon_{n+1}$$ 很可能落入其中。\n",
    "\n",
    "但是，$\\alpha, \\beta$ 和 $\\sigma^2$ 是未知的，因此我们使用预测值构建预测区间 $$\\hat{Y} = \\mu(x_{n+1}) = \\hat{\\alpha} + \\hat{\\beta}(x_{n+1} - \\bar{x})$$ 在 $x = x_{n+1}$ 处。\n",
    "\n",
    "考虑 $Y_{n+1}$ 和 $\\hat{Y}$ 的分布 $$ Y_{n+1} \\sim N\\left(\\alpha + \\beta(x_{n+1} - \\bar{x}), \\sigma^2\\right), $$ $$ \\hat{Y} \\sim N\\left(\\alpha + \\beta(x_{n+1} - \\bar{x}), \\sigma^2\\left[\\frac{1}{n} + \\frac{(x_{n+1} - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\right]\\right). $$ 由于 $\\epsilon_{n+1}$ 与 $\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n$ 相互独立，因此 $Y_{n+1}$ 和 $\\hat{Y}$ 也相互独立。\n",
    "\n",
    "因此，我们有 $$ Y_{n+1} - \\hat{Y} \\sim N\\left(0, \\sigma^2\\left[1 + \\frac{1}{n} + \\frac{(x_{n+1} - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\right]\\right). $$\n",
    "\n",
    "$Y_{n+1}$ 的双边 $100(1-\\alpha)\\%$ 预测区间为 $$ \\hat{\\alpha} + \\hat{\\beta}(x_{n+1} - \\bar{x}) \\pm t_{\\alpha/2}(n-2)S_R\\sqrt{1 + \\frac{1}{n} + \\frac{(x_{n+1} - \\bar{x})^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}} $$\n",
    "\n",
    "可以看出，和置信区间相比，预测区间更宽，因为它考虑了未来观测值的额外变异性。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
